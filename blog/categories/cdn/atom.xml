<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: cdn | Grenade Sandwich]]></title>
  <link href="http://smerrill.github.io/blog/categories/cdn/atom.xml" rel="self"/>
  <link href="http://smerrill.github.io/"/>
  <updated>2014-06-01T12:16:49-04:00</updated>
  <id>http://smerrill.github.io/</id>
  <author>
    <name><![CDATA[Steven W. Merrill]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Avoiding Flat Tires in Your Web Application]]></title>
    <link href="http://smerrill.github.io/blog/2013/06/03/avoiding-flat-tires/"/>
    <updated>2013-06-03T00:00:00-04:00</updated>
    <id>http://smerrill.github.io/blog/2013/06/03/avoiding-flat-tires</id>
    <content type="html"><![CDATA[<p>This Monday, the <a href="http://www.citibikenyc.com/?ef_id=IBFQPPF0mFIAAFdi:20130531201408:s">CitiBike bike share</a> launched in New York City. The website was beautiful and responsive, and more than 15,000 registrations were processed through it before the launch happened.</p>

<p>But then, a funny thing happened. The website and the mobile apps' maps started coming up blank, and they stayed blank for more than 12 hours. What follows should not be characterized as a failing of the technical team. Launches are tough, and I don&rsquo;t mean to pile on what was obviously a tough situation. Instead, I would like to look at a few choices that were made and how the system might have been better architected for scalability.</p>

<!--more-->


<h2>Architecture Spelunking</h2>

<p>I have no intimate knowledge of the architecture of the Citi Bike NYC application and hosting infrastructure, but we can find out a couple things very quickly.</p>

<p>The misbehaving path in question was <code>http://www.citibikenyc.com/stations/json</code>. With that information in hand, we can use the <code>dig</code> and <code>curl</code> commands to get a lot of information.
<code>
┌┤smerrill@lilliputian-resolution [May 30 18:23:35] ~
└╼ dig +short www.citibikenyc.com
70.32.89.47
70.32.83.162
</code></p>

<p>It looks like there&rsquo;s two web servers, and the site is using DNS-based load balancing. Let&rsquo;s take a look at the (now-fixed) JSON feed with <code>curl -v</code>. Using <code>curl -v</code> instead of <code>curl -I</code> ensures that we actually send a GET request instead of a HEAD request, just to make sure that the behavior is as close to a real browser as possible.</p>

<p>```
┌┤smerrill@lilliputian-resolution [May 31 12:11:59] ~
└╼ curl -v www.citibikenyc.com/stations/json > /dev/null
* About to connect() to www.citibikenyc.com port 80 (#0)
* Trying 70.32.83.162&hellip;
% Total % Received % Xferd Average Speed Time Time Time Current
Dload Upload Total Spent Left Speed
0 0 0 0 0 0 0 0 &mdash;:&mdash;:&mdash; &mdash;:&mdash;:&mdash; &mdash;:&mdash;:&mdash; 0* connected
* Connected to www.citibikenyc.com (70.32.83.162) port 80 (#0)</p>

<blockquote><p>GET /stations/json HTTP/1.1
User-Agent: curl/7.24.0 (x86_64-apple-darwin12.0) libcurl/7.24.0 OpenSSL/0.9.8r zlib/1.2.5
Host: www.citibikenyc.com
Accept: <em>/</em></p>

<p>&lt; HTTP/1.1 200 OK
&lt; Server: nginx
&lt; Date: Fri, 31 May 2013 16:11:59 GMT
&lt; Content-Type: text/html
&lt; Transfer-Encoding: chunked
&lt; Connection: keep-alive
&lt; Vary: Accept-Encoding
&lt; Set-Cookie: ci_session=5mh4HirmjGbrsFaoFoNf5I3MOe5%2FgpGTtterFR5ATyZIoSQewcZTqEk8CmTo1Y6A%2Bv29mRsaV7wmtMDot42z3Qo5Om5MBEVIWhVCLsBGjSWWNmyFXc4UVbNpLKIUYM3
LjeP08uWGQCw642sOgQaaZURYQlUoyBx%2F6KffPECV7IPgT7Lw8G%2FUJYzMDUHXdQDzfRenxAuMZmLpt%2BBWxUEZqCs87VWaYzhQEFnXwxSAcm4VtowNMdZZHfc8Rcw%2FSWzL4z6zJZlDhzYG0Lp%2B%
2F7rBv1wwBpqsCcSS8cXBNSW0XZc7VHJ2AuB%2BOvXzoLWwKMMH%2FHd%2FTI%2B%2BIH5Ec4O8Jjup%2Bg%3D%3D; expires=Fri, 31-May-2013 16:21:54 GMT; path=/
&lt; Vary: Accept-Encoding,User-Agent
&lt; X-Apache-Server: www1.citibikenyc.com
&lt; MS-Author-Via: DAV
&lt;
{ [data not shown]
100 116k 0 116k 0 0 853k 0 &mdash;:&mdash;:&mdash; &mdash;:&mdash;:&mdash; &mdash;:&mdash;:&mdash; 993k
* Connection #0 to host www.citibikenyc.com left intact
* Closing connection #0
```</p></blockquote>

<p>Just based on that, it looks like we can deduce several salient points about the architecture:</p>

<ul>
<li>There are two DNS addresses for the backend web servers, www1.citibikenyc.com and www2.citibikenyc.com.</li>
<li>The www.citibikenyc.com DNS entry contains both the www1 and www2 addresses.</li>
<li>The servers appear to be running nginx on port 80 and then connecting to a local Apache server based on the <code>X-Apache-Server</code> header.</li>
<li>The site is built using the CodeIgniter PHP framework (the main site appears to be using the Fuel CMS atop CodeIgniter.)</li>
<li>A quick trip to <a href="http://whois.arin.net/">arin</a> lets us know that those two IP addresses point to machines in a MediaTemple datacenter, so no CDN is in play.</li>
<li>Each request to the <code>/stations/json</code> path returns a Set-Cookie header that sets a CodeIgniter session cookie with a 10 minute lifetime.
For comparison, this is what the response looked like during the outage. The main differences are that during the outage, the feed was still returning a 200 HTTP response code, but the Content-Length was 0, and indeed the response body was blank, like so:</li>
</ul>


<p>```
┌┤smerrill@lilliputian-resolution [May 27 11:34:46] ~
└╼ curl -v www.citibikenyc.com/stations/json > /dev/null
* About to connect() to www.citibikenyc.com port 80 (#0)
* Trying 70.32.83.162&hellip;
% Total % Received % Xferd Average Speed Time Time Time Current
Dload Upload Total Spent Left Speed
0 0 0 0 0 0 0 0 &mdash;:&mdash;:&mdash; &mdash;:&mdash;:&mdash; &mdash;:&mdash;:&mdash; 0* connected
* Connected to www.citibikenyc.com (70.32.83.162) port 80 (#0)</p>

<blockquote><p>GET /stations/json HTTP/1.1
User-Agent: curl/7.24.0 (x86_64-apple-darwin12.0) libcurl/7.24.0 OpenSSL/0.9.8r zlib/1.2.5
Host: www.citibikenyc.com
Accept: <em>/</em></p>

<p>&lt; HTTP/1.1 200 OK
&lt; Server: nginx
&lt; Date: Mon, 27 May 2013 15:34:47 GMT
&lt; Content-Type: text/html
&lt; Content-Length: 0
&lt; Connection: keep-alive
&lt; Vary: Accept-Encoding
&lt; Set-Cookie: ci_session=x7ymouLWLEeY6%2Fo%2BudFoQHOixWyP3b9Ygp8Ocv94roUESql6Gwet1nuCVBcILmqt9DQzbFsSLLkXyOZ5qL%2Fl%2FD88F0Q0uXeLptE3zlGHxP0EISPGk5gW91SVscxi1klVRYv5Mt5zTO0KzB4obwc%2FY1AUFEodhplKXeaSURPXAw7roZVumXkmM1ALGbWQx5FF6LKm%2FtzudHm8NQPJYXDx3s3sUdVNWvWQpWe3iKEE5Su0TzqCKZcBxWYcssuPNVGEx8c5SpijHw6iR7sqnTMBnMdv7m4jsuj9ZweGk6JfGEp3G5%2BXAMqdsWfE%2Fa8449o92%2BlLug0NCFRdxH7ViZHXBA%3D%3D; expires=Mon, 27-May-2013 15:44:47 GMT; path=/
&lt; Vary: Accept-Encoding,User-Agent
&lt; X-Apache-Server: www1.citibikenyc.com
&lt; MS-Author-Via: DAV
&lt;
{ [data not shown]
0 0 0 0 0 0 0 0 &mdash;:&mdash;:&mdash; &mdash;:&mdash;:&mdash; &mdash;:&mdash;:&mdash; 0
* Connection #0 to host www.citibikenyc.com left intact
* Closing connection #0&lt;
```</p></blockquote>

<h2>The Case of the Missing JSON</h2>

<p>It seems the root of the problem was that the Apache server behind the nginx server was overloaded. It appears that at one point, the <code>/stations/json</code> endpoint was throwing errors based on this tweet:</p>

<blockquote><p>@<a href="https://twitter.com/stevenmerrill">stevenmerrill</a> That @<a href="https://twitter.com/citibikenyc">citibikenyc</a> feed has been returning Apache &ldquo;too many connections&rdquo; errors for a couple of hrs.Shows strong demand.</p>

<p>— codeline telemetry (@codelinegeekery) <a href="https://twitter.com/codelinegeekery/status/339033281687343105">May 27, 2013</a>
&nbsp;</p></blockquote>

<p>And then it was changed to this odd 200 response with no data after that. Naturally, that lead me to a simple question.</p>

<h2>Y u no cache?</h2>

<p>Why not cache or pre-generate the JSON? Even with a TTL of 60 or 30 seconds, caching the rendered JSON instead of going back to a dynamic application for every visitor could help protect the backend LAMP stack from spikes in traffic.</p>

<p>I can only speculate, but I imagine that the design of this feed went something like the following:</p>

<p>&ldquo;Our CitiBike stations will be pinging in every 30 seconds with new information about how many bikes and docks are available, so we have to be sure that the information we send to our mobile apps is as fresh as possible.&rdquo;</p>

<p>Keeping your customers up to date about if they&rsquo;ll be able to get or return a bike is certainly of paramount importance to a system like this, but that doesn&rsquo;t have to translate into dynamically generating JSON data on every request.</p>

<h2>Cache Rules Everything Around Me</h2>

<p>My first thought upon seeing this situation was hasty tweet:</p>

<blockquote><p>Oddly, the misbehaving @<a href="https://twitter.com/citibikenyc">citibikenyc</a> JSON feed returns a 200 and an empty body. Maybe the team should have written flat files and used a CDN.</p>

<p>— Steven Merrill (@stevenmerrill) <a href="https://twitter.com/stevenmerrill/status/339031661952004096">May 27, 2013</a>
&nbsp;</p></blockquote>

<p>There are several possible ways that the JSON response could be cached to lower the load on the backend web servers.</p>

<p>The lowest-risk option would be to have an out-of-band process like cron or Jenkins generate a static JSON file on the filesystem every 30 or 60 seconds. Then have nginx deliver that static JSON file to clients when they visit the <code>/stations/json</code> URL. This is a great option, since the load generated to service that endpoint would be both consistently timed (every 30 or 60 seconds) and predictable. For even more scalability beyond the two nginx servers, this file could then be pushed to a CDN, or the file could be served from <a href="http://www.fastly.com/">Fastly</a>, a Varnish-powered CDN with instant purging.</p>

<p>Another option that would allow for more finely-tuned updates is to use a message queueing system. With this design, running a task on cron or with Jenkins would not be needed. Instead, the bike docking stations would send messages to the queue whenever an action occurred that changed the number of bikes at the station (bike checked out or bike returned.) In a naive version of the system, each of these state-change messages would regenerate the whole JSON file. In a smarter version, the state-change messages would update a single cached JSON fragment representing the station, and the new JSON file could be generated from this cache so that database load would be lowered when the file had to be regenerated.</p>

<p>Both of these approaches are much preferred because the system load is controllable and is not tied to how many requests for the JSON file are coming in. A third server that didn&rsquo;t respond to web requests could do this processing.</p>

<p>Finally, since the application already has nginx at the frontend, another option is to use nginx&rsquo;s <code>proxy_cache</code> and <code>proxy_cache_use_stale</code> directives and cache the output of <code>/stations/json</code> for a certain TTL. This is not quite as ideal, as the potential exists to have TTL drift between the two web servers, such that someone might get data that was a few seconds stale based on which server they hit, but the business would probably agree that that is better than outright downtime.</p>

<p>For this to work, the Set-Cookie header must be removed. As far as I can tell, there is no personalized data being sent from that endpoint, so this session cookie does not serve a useful purpose.</p>

<h2>In Summary</h2>

<p>The launch of a web application can be tough. Load patterns emerge in the real world that your load testing didn&rsquo;t take into account. Regardless, if you know that any part of your system may get a lot of traffic or very bursty traffic, you can engineer your application for resilience by moving data generation out of uncached web requests and into cron- or queue-backed update processes.</p>

<p>Phase2 recently experienced a challenge like this in helping the <a href="//www.phase2technology.com/client/robin-hood-foundation/">Robin Hood Foundation</a> architect a website for the 12-12-12 Concert for Hurricane Sandy. If you&rsquo;re interested in learning more about that you can listen to the <a href="http://www.youtube.com/watch?v=itKY_lO7_Us&amp;amp;feature=youtu.be">panel talk we just gave at Drupalcon Portland</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Everything Old is New Again]]></title>
    <link href="http://smerrill.github.io/blog/steven/2009/10/12/everything-old-new-again/"/>
    <updated>2009-10-12T00:00:00-04:00</updated>
    <id>http://smerrill.github.io/blog/steven/2009/10/12/everything-old-is-new-again</id>
    <content type="html"><![CDATA[<p>It's time for my quarterly blogging drive, and to start, here's some information on my ever-increasing need to try out cool web technologies.</p>


<!--more-->


<h3>Server Migration</h3>




<p>My previous server was a VPS with 1 GB RAM for an obscenely low price from Serve By Design. I'm not linking to them, because I wouldn't recommend them, as you'll see.  For the money, I didn't expect them to stay around forever, and I was right. At the end of September, I got an email saying that they had to immediately cease all hosting and that I had 10 days to move my VPS.   Not great, but I can deal with that.</p>




<p>I've been doing some work with <a href="http://www.rackspacecloud.com/">Rackspace's cloud products</a> recently for work, so I decided to move to a Cloud Server VPS running CentOS.  I'm quite impressed with the Rackspace Cloud thus far, they have a robust backup / cloning system, and the plans are plenty cheap so long as your data transfer isn't too high.  Since I'm using <a href="http://www.simplecdn.com/">SimpleCDN</a> to handle all static content for my Drupal 6 sites, there's really nothing to worry about on the data transfer front.</p>




<p>But why stop with a new VPS and a new provider?  I decided to switch to some of the alternate technologies that people go to for maximum throughput on a Drupal site.  In this case, I'm not yet talking about Varnish and/or Pressflow (although I am a fan and user of both those projects in production on bigger sites.)  I decided to use the alternative webserver <a href="http://wiki.nginx.org/">nginx</a> and to run PHP dameonized through <a href="http://php-fpm.org/Main_Page">PHP-FPM</a>.</p>




<p>The installation of PHP-FPM and nginx was a snap. I initially read <a href="http://adityo.blog.binusian.org/?p=428">a great tutorial on how to do so on CentOS</a>, but then I found <a href="http://centos.alt.ru/pub/repository/centos/">the CentALT repo</a>, which has builds of nginx and PHP-FPM for your <strong>yum install</strong>ing pleasure.</p>




<p>Thus far, nginx and php-fpm have been rock-solid and quite fast (although I'm not going to break any traffic records with my hosted sites.)  I also like the ability to reload php configuration changes just by restarting the php-fpm service, and nginx restarts in milliseconds.</p>




<h3>Version Control</h3>




<p>I've been flirting with DVCS systems for while now.  I've mostly been looking at <a href="http://bazaar-vcs.org/en/">bzr</a>, because it's cross-platform, easy for people familiar with svn and cvs to grok, and supposedly offers the best Windows support of the lot.  It also has a Drupal upstream branch, and <a href="https://launchpad.net/pressflow">Pressflow</a> is now being managed on Launchpad.</p>




<p>There's now a nice <a href="http://bazaar-vcs.org/MacOSXDownloads">Snow Leopard-friendly installer</a> for bzr 2.0, and so I've been using that on my OS X boxes.</p>




<p>One of the neat features about bzr is that it can run off of dumb servers.  All you need is an SFTP server and you can host your own bzr branches.  Nonetheless, you can save on bandwidth (and score more geek cred) if you run your own smart server, so I set out to set up bzr and some of the cooler tools from the bzr ecosystem on my CentOS box.</p>




<p>Version 1.3 is the latest bzr version in any mainstream CentOS repo, so I built bzr 2.0 on my box.  I also got <a href="https://launchpad.net/loggerhead">Loggerhead</a> up and running for nice-looking graphical representations of my bzr branches like the one below.</p>




<p><a href="http://www.flickr.com/photos/00sven/4004158545" title="/lift-url-shortener : revision 7" class="flickr-photo-img"><img src="http://farm3.static.flickr.com/2642/4004158545_8dfb8177a7.jpg" alt="/lift-url-shortener : revision 7" title="/lift-url-shortener : revision 7"  class=" flickr-photo-img" height="411" width="500" /></a></p>




<p>Check back later in the week where I'll post more details about how to set up <strong>bzr serve</strong> as a CentOS service, and how to get Loggerhead via an nginx proxy to play nicely together.</p>

]]></content>
  </entry>
  
</feed>
